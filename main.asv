clear;
clc;
Z=load('hw4_nnet_train.txt');
for i=1:size(Z,2)-1
   X(:,i)=Z(:,i);
end
train_size=size(Z,1);
XX=Z;
X=load('hw4_nnet_test.txt');
Y=X(:,size(X,2));
out_size=size(X,1);






for z=8:1:8
    z=z
    repeat=500;
    M=[2 z 3 1];
    layer=size(M,2)-1;
    ra=0.1;
    n=0.01;
    clear W S De
    W=cell(1,layer);
    S=cell(1,layer+1);
    De=cell(1,layer);
    err=0;
    for r=1:repeat
        r=r
        % initial weight
        for k=1:layer
            pre_num=M(k);
            back_num=M(k+1);
            clear w;
            for j=1:back_num
                for i=1:pre_num+1
                    w(i,j)=rand*2*ra-ra;
                    %w(i,j)=1;
                end
            end
            W{1,k}=w;
            clear w;
        end
        for T=1:50000
            interation=T;
            input=XX(randi(train_size,1,1),:);
            yn=input(size(XX,2));
            input=input(1:size(XX,2)-1);
            S{1,1}=[1; input'];
            % forward
            for k=1:layer
                pre_num=M(k);
                back_num=M(k+1);
                w=W{1,k};
                s=[];
                B=S{1,k};
                for j=1:back_num
                    A=w(:,j);
                    score=tanh(A'*B);
                    s=[s score];
                end
                if k ~= layer
                s=[1 s];
                end
                S{1,k+1}=s';
                s=[];
            end
            
            %backward
            for k=layer:-1:1
                pre_num=M(k);
                back_num=M(k+1);
                s_b=S{1,k+1};
                s_p=S{1,k};
                w=W{1,k};
                if k == layer
                    d=-2*(yn-s_b)*(1-s_b^2);
                    for j=1:back_num
                        for i=1:pre_num+1
                            w(i,j)=w(i,j)-n*s_p(i)*d(j);
                        end
                    end
                    De{1,k}=d;
                    W{1,k}=w;
                else
                    d_b=De{1,k+1};
                    w_b=W{1,k+1};
                    n_last=M(k+2);
                    d=[];
                    for i=2:back_num+1
                        temp_sum=0;
                        for j=1:n_last
                            temp_sum=temp_sum+d_b(j)*w_b(i,j);
                            %e=s_b(i);
                            %d(i-1)=w_b(i,j)*d_b(j)*sech(s_b(i))^2;
                        end
                        d(i-1)=temp_sum*(1-s_b(i)^2);
                    end
                    De{1,k}=d';
                    for j=1:back_num
                        for i=1:pre_num+1
                            w(i,j)=w(i,j)-n*s_p(i)*d(j);
                        end
                    end
                    W{1,k}=w;
                end
            end
            
        end
        %clearvars -except W layer M err repeat n r XX train_size
        for i=1:out_size
            S_E=cell(1,layer+1);
            input=X(i,:);
            yn=input(size(X,2));
            input=input(1:size(X,2)-1);
            S_E{1,1}=[1; input'];
            for k=1:layer
                pre_num=M(k);
                back_num=M(k+1);
                w=W{1,k};
                s=[];
                B=S_E{1,k};
                for j=1:back_num
                    A=w(:,j);
                    score=tanh(A'*B);
                    s=[s score];
                end
                if k == layer
                    if sign(s) ~= yn
                        err=err+1;
                    end
                else
                    s=[1 s];
                    S_E{1,k+1}=s';
                end
            end
        end
        err=err
    end
    
    err=err/(out_size*repeat)
    
    clear S_E De 
end
%err=err/train_size;






